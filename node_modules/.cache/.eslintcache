[{"/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/from_scratch/src/index.js":"1","/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/index.js":"2","/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/serviceWorkerRegistration.js":"3","/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/reportWebVitals.js":"4","/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/App.js":"5","/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/utilities.js":"6","/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/service-worker.js":"7"},{"size":853,"mtime":1674289680913,"results":"8","hashOfConfig":"9"},{"size":853,"mtime":1674289680913,"results":"10","hashOfConfig":"11"},{"size":5064,"mtime":1674289680918,"results":"12","hashOfConfig":"11"},{"size":364,"mtime":1674289680916,"results":"13","hashOfConfig":"11"},{"size":4183,"mtime":1674313699430,"results":"14","hashOfConfig":"11"},{"size":10178,"mtime":1674313191011,"results":"15","hashOfConfig":"11"},{"size":2837,"mtime":1674289680917,"results":"16","hashOfConfig":"11"},{"filePath":"17","messages":"18","suppressedMessages":"19","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"20"},"t96nni",{"filePath":"21","messages":"22","suppressedMessages":"23","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},"1ms51l2",{"filePath":"24","messages":"25","suppressedMessages":"26","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"27","messages":"28","suppressedMessages":"29","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"30","messages":"31","suppressedMessages":"32","errorCount":0,"fatalErrorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":"33"},{"filePath":"34","messages":"35","suppressedMessages":"36","errorCount":0,"fatalErrorCount":0,"warningCount":4,"fixableErrorCount":0,"fixableWarningCount":0,"source":"37"},{"filePath":"38","messages":"39","suppressedMessages":"40","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},"/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/from_scratch/src/index.js",[],[],[],"/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/index.js",[],[],"/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/serviceWorkerRegistration.js",[],[],"/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/reportWebVitals.js",[],[],"/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/App.js",["41","42","43"],[],"import React,{useRef, useEffect,useState} from 'react';\nimport './App.css';\nimport * as poseDetection from '@tensorflow-models/pose-detection';\nimport * as tf from '@tensorflow/tfjs-core';\n// Register one of the TF.js backends.\nimport '@tensorflow/tfjs-backend-webgl';\nimport { classify, drawKeypoints, drawSkeleton } from \"./utilities\";\n//import mp3 from \"./mp3\"\n\n\nfunction App() {\n  let lastTrueTime = Date.now();\n  const videoRef = useRef(null);\n  const canvasRef = useRef(null);\n  const soundContext = require.context('./mp3', true, /\\.mp3$/);\n  const sounds = soundContext.keys().reduce((sounds, soundFile) => {\n    sounds[soundFile] = soundContext(soundFile);\n    return sounds;\n  }, {});\n\n  const soundFiles = [\n    './straight.mp3',\n    './legs_apart.mp3',\n    './legs_bend.mp3',\n    './elbows_bend.mp3',\n    './sholders_bend.mp3',\n    './hips_bend.mp3',\n    './hollowback.mp3'\n  ];\n\n  // Load MoveNet\n  function play_sound(label){\n    console.log(soundFiles[label.findIndex(x => x === 1)]);\n    new Audio(sounds[soundFiles[label.findIndex(x => x === 1)]]).play()\n  }\n\n\n  const runMoveNet = async () => {\n    const detectorConfig = {modelType: poseDetection.movenet.modelType.SINGLEPOSE_THUNDER};\n    const net = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, detectorConfig);\n    //\n    setInterval(() => {\n      detect(net);\n    }, 100);\n  };\n\n  \n\n\n  const detect = async (net) => {\n    if (\n      typeof videoRef.current !== \"undefined\" &&\n      videoRef.current !== null\n    ) {\n      // Get Video Properties\n      const video = videoRef.current;\n      const videoWidth = videoRef.current.videoWidth;\n      const videoHeight = videoRef.current.videoHeight;\n\n      // Set video width\n      videoRef.current.width = videoWidth;\n      videoRef.current.height = videoHeight;\n\n      // Make Detections\n      const pose = await net.estimatePoses(video);\n\n      if (Date.now() - lastTrueTime > 3000) {\n      \n      \n      //Check if whole body is visible\n      let k = pose[0][\"keypoints\"]\n      const label = classify(pose[0][\"keypoints\"]);\n      if (pose[0][\"keypoints\"].every(keypoint => keypoint.score > 0.1)){\n        if (k[16].y<k[6].y || k[15].y<k[5].y){\n          //const label = classify(pose[0][\"keypoints\"]);\n          console.log(label)\n\n          if (Date.now() - lastTrueTime > 2000) {\n            // Your code to be executed every 3 seconds\n            //console.log(\"condition is true\");\n            \n            play_sound(label);\n          }\n        }\n      }\n      lastTrueTime = Date.now();\n      }\n      \n\n      \n      \n\n      drawCanvas(pose[0], video, videoWidth, videoHeight, canvasRef);\n    }\n  };\n\n  const drawCanvas = (pose, video, videoWidth, videoHeight, canvas) => {\n    const ctx = canvas.current.getContext(\"2d\");\n    canvas.current.width = videoWidth;\n    canvas.current.height = videoHeight;\n\n\n    drawKeypoints(pose[\"keypoints\"], 0.3, ctx);\n    drawSkeleton(pose[\"keypoints\"], 0.3, ctx);\n  };\n\n\n  const getVideo = () => {\n    navigator.mediaDevices\n      .getUserMedia({ \n        video: {facingMode: \"user\", width: 480, height: 640 }\n      }).then(stream => {\n        let video = videoRef.current;\n        video.srcObject = stream;\n        video.play();\n        console.log(video)\n      })\n      .catch(err => {\n        console.error(err);\n      })\n  }\n\n  useEffect(() => {\n    getVideo();\n    runMoveNet();\n\n  }, [videoRef])\n\n  \n\n\n  return (\n    <div className=\"App\">\n      <div className=\"camera\">\n        <video\n          ref={videoRef}\n          style={{\n            position: \"absolute\",\n            marginLeft: \"auto\",\n            marginRight: \"auto\",\n            left: 0,\n            right: 0,\n            textAlign: \"center\",\n            zindex: 9,\n            width: 480,\n            height: 640,\n          }}\n        />\n\n        <canvas\n          ref={canvasRef}\n          style={{\n            position: \"absolute\",\n            marginLeft: \"auto\",\n            marginRight: \"auto\",\n            left: 0,\n            right: 0,\n            textAlign: \"center\",\n            zindex: 9,\n            width: 480,\n            height: 640,\n          }}\n        />\n\n      </div>\n    </div>\n  );\n}\n\nexport default App;\n","/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/utilities.js",["44","45","46","47"],[],"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as poseDetection from '@tensorflow-models/pose-detection';\nimport * as tf from '@tensorflow/tfjs-core';\n\nconst color = \"aqua\";\nconst boundingBoxColor = \"red\";\nconst lineWidth = 2;\n\nexport const tryResNetButtonName = \"tryResNetButton\";\nexport const tryResNetButtonText = \"[New] Try ResNet50\";\nconst tryResNetButtonTextCss = \"width:100%;text-decoration:underline;\";\nconst tryResNetButtonBackgroundCss = \"background:#e61d5f;\";\n\nfunction isAndroid() {\n  return /Android/i.test(navigator.userAgent);\n}\n\nfunction isiOS() {\n  return /iPhone|iPad|iPod/i.test(navigator.userAgent);\n}\n\nexport function isMobile() {\n  return isAndroid() || isiOS();\n}\n\nfunction setDatGuiPropertyCss(propertyText, liCssString, spanCssString = \"\") {\n  var spans = document.getElementsByClassName(\"property-name\");\n  for (var i = 0; i < spans.length; i++) {\n    var text = spans[i].textContent || spans[i].innerText;\n    if (text == propertyText) {\n      spans[i].parentNode.parentNode.style = liCssString;\n      if (spanCssString !== \"\") {\n        spans[i].style = spanCssString;\n      }\n    }\n  }\n}\n\nexport function updateTryResNetButtonDatGuiCss() {\n  setDatGuiPropertyCss(\n    tryResNetButtonText,\n    tryResNetButtonBackgroundCss,\n    tryResNetButtonTextCss\n  );\n}\n\n/**\n * Toggles between the loading UI and the main canvas UI.\n */\nexport function toggleLoadingUI(\n  showLoadingUI,\n  loadingDivId = \"loading\",\n  mainDivId = \"main\"\n) {\n  if (showLoadingUI) {\n    document.getElementById(loadingDivId).style.display = \"block\";\n    document.getElementById(mainDivId).style.display = \"none\";\n  } else {\n    document.getElementById(loadingDivId).style.display = \"none\";\n    document.getElementById(mainDivId).style.display = \"block\";\n  }\n}\nfunction toTuple({ y, x }) {\n  return [y, x];\n}\n\nexport function drawPoint(ctx, y, x, r, color) {\n  ctx.beginPath();\n  ctx.arc(x, y, r, 0, 2 * Math.PI);\n  ctx.fillStyle = color;\n  ctx.fill();\n}\n\n/**\n * Draws a line on a canvas, i.e. a joint\n */\n\n\nexport function classify(keypoints, tolerance = 30, max_dist = 0.05*480) {\n  const facing_right = facing_dir(keypoints);\n  const label = [0, 0, 0, 0, 0, 0, 0]; // straight, legs_apart, legs bend, elbows_bend, sholder_bend, hip_bend, hollowback\n\n  // angles right\n  const angleKneeRight = get_angle(keypoints[16], keypoints[14], keypoints[12], facing_right);\n  const angleHipRight = get_angle(keypoints[14], keypoints[12], keypoints[6], facing_right);\n  const angleSholderRight = get_angle(keypoints[12], keypoints[6], keypoints[8], facing_right);\n  const angleElbowRight = get_angle(keypoints[6], keypoints[8], keypoints[10], facing_right);\n  // angles left\n  const angleKneeLeft = get_angle(keypoints[15], keypoints[13], keypoints[11], facing_right);\n  const angleHipLeft = get_angle(keypoints[13], keypoints[11], keypoints[5], facing_right);\n  const angleSholderLeft = get_angle(keypoints[11], keypoints[5], keypoints[7], facing_right);\n  const angleElbowLeft = get_angle(keypoints[5], keypoints[7], keypoints[9], facing_right);\n\n  console.log(\"angleKneeRight  \"+angleKneeRight)\n  console.log(\"angleHipRight  \"+angleHipRight)\n  console.log(\"angleSholderRight  \"+angleSholderRight)\n  console.log(\"angleKneeLeft  \"+angleKneeLeft)\n  console.log(\"angleHipLeft  \"+angleHipLeft)\n  console.log(\"angleSholderLeft  \"+angleSholderLeft)\n  console.log(\"angleElbowLeft  \"+angleElbowLeft)\n\n  \n  if (angleKneeRight > 180+tolerance*2 || angleKneeLeft > 180+tolerance*2) {\n    label[2] = 1;        //legs_bend\n} else if ((keypoints[15].x-keypoints[16].x) > max_dist || (keypoints[15].x-keypoints[16].x) < -max_dist) {\n    label[1] = 1;        //legs_apart\n} else if (angleElbowRight > 180+tolerance*2 || angleElbowLeft > 180+tolerance*2) {\n    label[3] = 1;        //elbows_bend\n} else if (angleHipRight > 180+tolerance/1.5 || angleHipLeft > 180+tolerance/1.5) {\n    label[6] = 1;        //hollowback\n} else if (angleSholderRight > 180+tolerance || angleSholderLeft > 180+tolerance) {\n    label[6] = 1;        //hollowback  \n} else if (angleHipRight < 180-tolerance/1.75 || angleHipLeft < 180-tolerance/1.75) {\n    label[5] = 1;        //hip_bend  \n} else if (angleSholderRight < 170-tolerance || angleSholderLeft < 170-tolerance) {\n    label[4] = 1;        //shoulder_bend  \n} else {\n    label[0] = 1;        //straight\n}\n  return label;\n  }\n\nexport function facing_dir(keypoints) {\n  //0=chest faces left / 1=chest faces right\n  if (keypoints[0].x < keypoints[1].x || keypoints[0].x < keypoints[2].x) {\n      return 0;\n  } else if (keypoints[0].x > keypoints[1].x || keypoints[0].x > keypoints[2].x) {\n      return 1;\n  }\n}\nexport function get_angle(point_1, point_2, point_3, facing) {\n  \n  let vector_21 = [point_2.x-point_1.x, point_2.y-point_1.y];\n  let vector_23 = [point_2.x-point_3.x, point_2.y-point_3.y];\n  //console.log(vector_21);\n  let cross_product = vector_21[0]*vector_23[1] - vector_21[1]*vector_23[0];\n  let dot_product = vector_21[0]*vector_23[0] + vector_21[1]*vector_23[1];\n  \n  let abs_vector_21 = Math.sqrt(vector_21[0]**2 + vector_21[1]**2);\n  let abs_vector_23 = Math.sqrt(vector_23[0]**2 + vector_23[1]**2);\n  \n  let sign = Math.atan2(cross_product,dot_product);\n  let angle = Math.acos(dot_product/(abs_vector_21*abs_vector_23));\n  angle = angle * 180 / Math.PI;\n  //0=fingers point to the right / 1=fingers point to the left\n  if (facing === 1) {\n      if (sign < 0) {\n          angle = 360-angle;\n      }\n  } else {\n      if (sign > 0) {\n          angle = 360-angle;\n      }\n  }\n  return angle;\n}\n\n\n/**\n * Draws a pose skeleton by looking up all adjacent keypoints/joints\n */\nexport function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {\n\n  const adjacentKeyPoints = poseDetection.util.getAdjacentPairs(poseDetection.SupportedModels.MoveNet);\n  //console.log(keypoints[0].score)\n  adjacentKeyPoints.forEach(([i, j]) => {\n\n    const kp1 = keypoints[i];\n    const kp2 = keypoints[j]; \n    \n\n    const score1 = kp1.score != null ? kp1.score : 1;\n    const score2 = kp2.score != null ? kp2.score : 1;\n\n    if (score1 >= minConfidence && score2 >= minConfidence) {\n      ctx.beginPath();\n      ctx.moveTo(kp1.x, kp1.y);\n      ctx.lineTo(kp2.x, kp2.y);\n      ctx.strokeStyle = color;\n      ctx.stroke();\n      }});\n}\n\n/**\n * Draw pose keypoints onto a canvas\n */\nexport function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {\n  \n  for (let i = 0; i < keypoints.length; i++) {\n    const keypoint = keypoints[i];\n\n    if (keypoint.score < minConfidence) {\n      continue;\n    }\n    //console.log(keypoint)\n    const { y, x } = keypoint;\n\n\n    drawPoint(ctx, y * scale, x * scale, 3, color);\n  }\n}\n\n/**\n * Draw the bounding box of a pose. For example, for a whole person standing\n * in an image, the bounding box will begin at the nose and extend to one of\n * ankles\n */\n\n\n/**\n * Converts an arary of pixel data into an ImageData object\n */\nexport async function renderToCanvas(a, ctx) {\n  const [height, width] = a.shape;\n  const imageData = new ImageData(width, height);\n\n  const data = await a.data();\n\n  for (let i = 0; i < height * width; ++i) {\n    const j = i * 4;\n    const k = i * 3;\n\n    imageData.data[j + 0] = data[k + 0];\n    imageData.data[j + 1] = data[k + 1];\n    imageData.data[j + 2] = data[k + 2];\n    imageData.data[j + 3] = 255;\n  }\n\n  ctx.putImageData(imageData, 0, 0);\n}\n\n/**\n * Draw an image on a canvas\n */\nexport function renderImageToCanvas(image, size, canvas) {\n  canvas.width = size[0];\n  canvas.height = size[1];\n  const ctx = canvas.getContext(\"2d\");\n\n  ctx.drawImage(image, 0, 0);\n}\n\n/**\n * Draw heatmap values, one of the model outputs, on to the canvas\n * Read our blog post for a description of PoseNet's heatmap outputs\n * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\n */\nexport function drawHeatMapValues(heatMapValues, outputStride, canvas) {\n  const ctx = canvas.getContext(\"2d\");\n  const radius = 5;\n  const scaledValues = heatMapValues.mul(tf.scalar(outputStride, \"int32\"));\n\n  drawPoints(ctx, scaledValues, radius, color);\n}\n\n/**\n * Used by the drawHeatMapValues method to draw heatmap points on to\n * the canvas\n */\nfunction drawPoints(ctx, points, radius, color) {\n  const data = points.buffer().values;\n\n  for (let i = 0; i < data.length; i += 2) {\n    const pointY = data[i];\n    const pointX = data[i + 1];\n\n    if (pointX !== 0 && pointY !== 0) {\n      ctx.beginPath();\n      ctx.arc(pointX, pointY, radius, 0, 2 * Math.PI);\n      ctx.fillStyle = color;\n      ctx.fill();\n    }\n  }\n}\n\n/**\n * Draw offset vector values, one of the model outputs, on to the canvas\n * Read our blog post for a description of PoseNet's offset vector outputs\n * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\n */\n// export function drawOffsetVectors(\n//     heatMapValues, offsets, outputStride, scale = 1, ctx) {\n//   const offsetPoints =\n//       posenet.singlePose.getOffsetPoints(heatMapValues, outputStride, offsets);\n\n//   const heatmapData = heatMapValues.buffer().values;\n//   const offsetPointsData = offsetPoints.buffer().values;\n\n//   for (let i = 0; i < heatmapData.length; i += 2) {\n//     const heatmapY = heatmapData[i] * outputStride;\n//     const heatmapX = heatmapData[i + 1] * outputStride;\n//     const offsetPointY = offsetPointsData[i];\n//     const offsetPointX = offsetPointsData[i + 1];\n\n//     drawSegment(\n//         [heatmapY, heatmapX], [offsetPointY, offsetPointX], color, scale, ctx);\n//   }\n// }\n","/Users/davidjaehrling/Desktop/Bachelor/JS-WebApp/final/src/service-worker.js",[],["48","49","50","51"],{"ruleId":"52","severity":1,"message":"53","line":1,"column":33,"nodeType":"54","messageId":"55","endLine":1,"endColumn":41},{"ruleId":"52","severity":1,"message":"56","line":4,"column":13,"nodeType":"54","messageId":"55","endLine":4,"endColumn":15},{"ruleId":"57","severity":1,"message":"58","line":127,"column":6,"nodeType":"59","endLine":127,"endColumn":16,"suggestions":"60"},{"ruleId":"52","severity":1,"message":"61","line":21,"column":7,"nodeType":"54","messageId":"55","endLine":21,"endColumn":23},{"ruleId":"52","severity":1,"message":"62","line":22,"column":7,"nodeType":"54","messageId":"55","endLine":22,"endColumn":16},{"ruleId":"63","severity":1,"message":"64","line":45,"column":14,"nodeType":"65","messageId":"66","endLine":45,"endColumn":16},{"ruleId":"52","severity":1,"message":"67","line":78,"column":10,"nodeType":"54","messageId":"55","endLine":78,"endColumn":17},{"ruleId":"68","severity":2,"message":"69","line":22,"column":18,"nodeType":"54","messageId":"70","endLine":22,"endColumn":22,"suppressions":"71"},{"ruleId":"68","severity":2,"message":"69","line":53,"column":31,"nodeType":"54","messageId":"70","endLine":53,"endColumn":35,"suppressions":"72"},{"ruleId":"68","severity":2,"message":"69","line":66,"column":1,"nodeType":"54","messageId":"70","endLine":66,"endColumn":5,"suppressions":"73"},{"ruleId":"68","severity":2,"message":"69","line":68,"column":5,"nodeType":"54","messageId":"70","endLine":68,"endColumn":9,"suppressions":"74"},"no-unused-vars","'useState' is defined but never used.","Identifier","unusedVar","'tf' is defined but never used.","react-hooks/exhaustive-deps","React Hook useEffect has a missing dependency: 'runMoveNet'. Either include it or remove the dependency array.","ArrayExpression",["75"],"'boundingBoxColor' is assigned a value but never used.","'lineWidth' is assigned a value but never used.","eqeqeq","Expected '===' and instead saw '=='.","BinaryExpression","unexpected","'toTuple' is defined but never used.","no-restricted-globals","Unexpected use of 'self'.","defaultMessage",["76"],["77"],["78"],["79"],{"desc":"80","fix":"81"},{"kind":"82","justification":"83"},{"kind":"82","justification":"83"},{"kind":"82","justification":"83"},{"kind":"82","justification":"83"},"Update the dependencies array to be: [runMoveNet, videoRef]",{"range":"84","text":"85"},"directive","",[3381,3391],"[runMoveNet, videoRef]"]